---
title: privacy-preserving deep-learning via weight Transmission
date: 2021-03-20 19:42:37
tags: privacy-preserving
mathjax: true
---
本文针对《Privacy-Preserving Deep Learning via Weight Transmission》该篇论文进行了分析

比较全面 

<!--more-->

### 论文题目理解

本文设立了一个能保护隐私的深度学习算法，根据本文的内容，我将原题目《Privacy-Preserving Deep Learning via Weight Transmission》扩展为《Privacy-preserving SGD for distributed trainers via weight transmission》即通过权重传输实现的针对分布式训练者的隐私保护随机梯度下降算法，通过对题目的分析，我们知道本文的书写环境和目的如下：

* 算法 ：SGD 随机梯度下降

* 目的 ：隐私保护

* 环境 ：分布式神经网路 & 深度学习

### 论文背景知识

#### 背景知识一：

本文是在SGD随机梯度下降算法的基础上进行改进和扩展的。为了介绍SGD，我们首先介绍一下GD最速梯度下降算法-Gradient Descent，该算法是通过计算给定数据集的导数，使函数不断收敛至J函数-cost function的最小值，得到结果，该方法的具体数学公式为
$$
x_{t+1} = x_t - \eta_t\nabla f(x_t)
$$
其中$\eta_t$为步长，  为$\nabla f(x_t)$导数，该方法有两个缺点：

1. 为了保证准确性$\nabla f(x_t)$需要是一个精确的导数，所以需要进行很多次迭代的运算才能得到结果
2. 该方法无法逃离鞍点和局部最优点这种导数为0但是不是最小值的点。

因此我们引入了SGD算法，该算法为随机最速下降算法-Stochastic Gradient Descent。他的主要思想是利用包含噪声的具有随机性的导数使得下降过程不经过鞍点和局部最优点。其中随机导数仅要求期望为单数即可，数学表达为  。该方法的全部数学表达为：
$$
G_t = \frac{\delta J(W_{t-1},X_t,Y_t)}{\delta W} \\\\
W_t = W_{t-1} - \alpha(t)·G_t
$$
其中 J 函数为cost function，即对结果优良性的衡量函数，W为权值参数，是每一个属性对结果的权重影响。$\alpha_t$为步长,$G_t$为随机导数。相比于GD，SGD有以下的优良特性：

1. 导数可以包含噪声，所以算得很快，且大量的理论工作说明，只要噪声不离谱，其实（至少在f是凸函数的情况下），SGD都能够很好地收敛。
2. 它能够自动逃离鞍点，自动逃离比较差的局部最优点。
3. 最后找到的答案还具有很强的一般性（generalization），即能够在自己之前没有见过但是服从同样分布的数据集上表现非常好。

这些优良的性质使得SGD成为深度学习领域较为普遍且优良的算法 ，这也是本文选择这个算法的原因之一。

本文还有其他有关隐私保护`Privacy-preserving`的背景知识为：

#### Honest-but-Curious

首先本文假定server是诚实但好奇的(honest-but-curious)，该模型又被称为半诚实模型(semi-Honest model)，其中

* 诚实意为结点会诚实的将他的工作全部完成，
* 好奇意味着结点在执行过程中会将他所有的数据全部保存下来并在后续时利用他们进行推测攻击。

#### Collusion

相关的，本文假定的攻击还有，trainer和server将会是一伙的，他们之间可以进行共谋(collusion)来对数据进行攻击。

### 论文的主要贡献

1. 舍弃了传统梯度传输的方法而选用权值传输来大幅提高安全性。
2. 本文使用的方法适用于深度学习适用的所有激活函数，这意味着不会适用近似算法。
3. 本文通过严格的理论证明诚实但好奇的server和极端共谋，即只有一个trainer可信的情况下，本文的方法依然有极高的安全性。
4. 通过一个证明表达了本文在准确性方面和将所有数据集集成在一个训练者训练的结果准确性是相同的。

## 论文主要内容

### 权值传输

首先，我们要为大家解释一下为什么权值传输相比梯度传输拥有更高的安全性和保护隐私性，本篇文章通过跟之前两篇文章的比较来说明。

第一篇的主要思想是只传输随机选择的部分梯度而放弃传统的加密方法，尽管在Sec.7 中文章作者又利用了差分隐私增加拉普拉斯噪声的方法，但是数据保密性和差分隐私是正交的，所以仍可能造成泄露。

> R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in Proc. 22nd ACM SIGSAC Conf. Comput. Commun. Secur., I. Ray, N. Li, and C. Kruegel, Eds., Oct. 2015, pp. 1310–1321.

第二篇文章主要是使用同态加密的方法，虽然进行了加密，但是在共谋的情况下，坏人对其进行解密并得到原始梯度，仍然会对数据造成泄露。所以梯度传输的方法总归会造成数据的泄露，我们选择权值传输的方法来避免这种泄露。这样做的具体原因我们会在theorem2 中进行讲解

> L. T. Phong, Y. Aono, T. Hayashi, L. Wang, and S. Moriai, “Privacypreserving deep learning via additively homomorphic encryption,” IEEE Trans. Inf. Forensics Security, vol. 13, no. 5, pp. 1333–1345, May 2018.

### SNT & FNT

本文提供了两种系统SNT，FNT其中大部分内容我们在SNT内就讲到所以我们会详细介绍SNT，并之后简要涉及FNT。我们首先明确在分布式学习过程中，每次进行一个大的循环即每个trainer都进行一次，包含对于每一个trainer的小循环。其中每个符号的意义及说明如下，

1.  server端是Honest-but-curious 的。
2. Dataseti 指代每个人能掌握的数据集。
3. （X,Y）是Dataset的子集，且根据分布不同可以是整个数据集或者随机选择的一部分。
4. K是trainer 间共享的密钥，但是不能告诉 server –FNT 就不需要了。
5. $Enc_k(W)$是用k密钥对W进行加密。
6. encW 是加密后的密文。
7.  $Dec_k(C)$是用K密钥对C进行解密。
8. $W_0$是随机选择的初始权重。其中除K以外的所有变量都是vector形式出现的。

每个trainer执行的操作如下:
$$
W_1=Dec_k(encW_1) \\\\
G_2 = \frac{\delta J(W_1,X_2,Y_2)}{\delta W} \\\\
W_2 = W_1 - \alpha(t)·G_2 \\\\
encW_2 = Enc_k(W_2)
$$
​      

然后将encW传给server，并由server端发送给下一个trainer。

{% asset_img snt.png snt %}

  **FNT****系统是全连接的,这就产生了一个问题，经过我们对于SNT的描述来看，我们可以以固定的顺序进行训练，那我们为什么需要全连接，增加这么多的路径呢，原因有以下三点：

1. 如果路径失效，那么系统可能会因此坏掉，所以全连接会增加可靠性。
2. 全连接的方法可以适应随机发送的工作模式，而不用只能按顺序发送。
3. 可以实现匿名传输的安全措施，这个我们接下来会讲。

{% asset_img fnt.png fnt %}

### 安全 及 准确性 理论解释

#### 理论一：系统在面对诚实但好奇的服务端下的安全

我们使用对称加密方法使server端只能看见加密后的W权重，我们选用能抵抗选择的对称加密方法即可。

#### 理论二：在面对极端共谋collusion下的安全性

极端共谋即当只有trainer1可信，trainer2-l,server都不可信的情况下,坏人也不能计算出trainer1的数据集信息，除非他们解决非线性问题或子集和问题

其中**非线性问题**代表我们在进行计算时，我们选择的激活函数都是非线性函数，如果我们想要从计算出来的权值恢复数据集的本身数据，那么我们就需要解决非线性问题，而这种问题是十分难解的

子集和问题的讲解我们需要借助一些数学公式，首先我们明确的是，在每个训练者经历的一次小循环中，他需要执行有多个公式组成的(1)号式子，并通过化简得到到（2）号式子，因为我们假定前后trainer知道初始的W和结束后的W，那么他未知的就是计算后的(3)号式子。
$$
G_1 = \frac{\delta J(W_{0},X_1,Y_1)}{\delta W} \\\\
W_1 = W_{0} - \alpha(1)·G_1 \\\\
\vdots \\\\
G_i = \frac{\delta J(W_{i-1},X_i,Y_i)}{\delta W} \\\\
W_i = W_{i-1} - \alpha(i)·G_i \\\\
\vdots \\\\
G_n = \frac{\delta J(W_{n-1},X_n,Y_n)}{\delta W} \\\\
W_n = W_{n-1} - \alpha(n)·G_n     (1)
$$

$$
W^{(final)} = W_n \\\\
= W_n-1 - \alpha_nG_n \\\\
= W_n-2 - \alpha_{n-1}G_{n-1} - \alpha_nG_n \\\\
\vdots \\\\
= w_0 - (\alpha_{1}G_{1} + \dots + \alpha_nG_n) \\\\
= W^{(init)} - (\alpha_{1}G_{1} + \dots + \alpha_nG_n) (2) \\\\
W^{(init)} - W^{(final)} = (\alpha_{1}G_{1} + \dots + \alpha_nG_n) (3)
$$

其中符号的定义如下

-  (𝑋_𝑖,𝑌_𝑖) (1≤𝑖≤𝑛)是Dataset1洗牌后的一小部分

-  𝛼_𝑖*可以是相同的，也可以是trainer1自己选择的*

-  𝑛 = |𝐷𝑎𝑡𝑎𝑠𝑒𝑡1|/𝑏𝑎𝑡𝑐ℎ_𝑠𝑖𝑧𝑒≫1

我们看到，我们已知（3）式的和，且n是十分大的。算得每个值是一个NP-Complete难解问题，需要多项式未知的时间复杂度。所以他是安全的。

#### 定理三：分布式的数据集的运算与在一个主机上对所有数据集的并集进行运算得到的结果是相同的。

  该定理在逻辑上理解就可以，文中也只给了描述性的证明

{% asset_img fig3.jpg fig3 %}

---

### 附加考虑 additional consideration

在该系统运用和实验实际执行的过程中，文中也增加了很多其他非主体系统涉及的方法。

#### 为了证明子集和问题的实验数据

在实际数据集中，未知数的数量是远大于等式的数量的，具体的实验情况见下图

{% asset_img table3.jpg table3 %}

#### 为了增加数据集的训练效果，我们会对数据集进行扩张

例如图片：利用旋转，裁剪，翻转等方法 , 过拟合后面会解决

#### 实际训练中使用了SGD的进化版本

例如 ：RMSProp [27] or Adam [28],

---

### 预定义 additional hedge

同时为了使系统更好的进行，我们还进行了一些预定义。首先我们需要明确想要达到完美的，没有泄露的系统是不可能的。文中将其描述为Dalenius desideratum，即系统将会转化为完全的随机数传输，这是没有丝毫实用性的。

#### 增加了差分隐私即添加噪声的方法

用于增加隐私性,其实就是在隐私性和准确性之间寻找一种平衡

#### 利用Dropout 的方法增加差分隐私和防止过拟合

Dropout 的方法由神经网络领域大牛Hinton 14年提出，主要做法是在训练时随机隐藏一些结点，这样能降低结点间的依赖性，同时也会降低对于数据的依赖性。

**举例**来说，我们将一支军队分为三部分分别在海陆空进行训练，那么他们最后组合起来环境对于他们影响会很小，同时我们也没有办法根据不同环境他们的作战表现判断他们是海军，陆军还是空军。

#### 使用匿名传输的方法增强隐私性

我们使用的具体方法为，在由server转发时隐藏数据的来源，或者随机传输，即一个数据可能来自小集合中的任一元素

#### 赋予每个结点当发现问题就进行防御性进攻的权力（即1，2，3）

#### 实际攻击举例

文中还用实际的攻击进行举例，该攻击的目的是通过获取的信息找到模型上一跳在哪个数据集训练的。

首先，我们当然可以利用1，2，4的方法综合起来，防止这种攻击。当面临一个更强的具体的攻击时，比如成员推理攻击，我们这样防范。

成员推理攻击的**目的**是：给出一条样本，可以推断该样本是否在模型的训练数据集中——即便对模型的参数、结构知之甚少，该攻击仍然有效

**本质**是：构建一个二分类模型，以X,Y为参数，判断该项是否在数据集中

**主要部分**是：利用影子模型(shadow model)构建与原目标模型训练集相似的数据集

文中给出了三种构建影子模型的方法：

**Model-based synthesis：** 直观上，如果目标模型以很高的概率给出了某条record的类别，那么该record与目标模型训练集中的数据应该是十分相似的。所以，可以用目标模型本身来构建影子模型的训练数据：

*  我们通过差分隐私添加噪声的方法使概率处于变化中，挑战他的阈值

**Statistics-based synthesis：** 攻击者知道目标模型训练数据的分布信息，比如feature的边缘分布，那么可以直接由分布生成数据。

**Noisy real data：** 攻击者也许可以获得和目标模型训练集数据相似的数据，可以认为是目标模型训练集的噪声版本，直接利用之

* 对于二和三方法，由于本文的数据集只会在本地被训练，所以攻击者既不能得到分布也不能得到带噪声的数据，因此可以防范这种攻击。

## 总结

本片文章亮点主要就是**权值传输**，并且真的是灵活利用了这个内容，我觉得它主要做到了两件事

* 首先它让每一个结点在一次循环里执行多次运算，这样就成功引入了**子集和问题**，相当于对于梯度的加密也是对于数据的二次加密
* 利用了权值的递归性，这样可以对权值进行加密，下一个trainer再进行解密后计算，中间的转发者不需要知道**密钥**也不需要知道内容，这可能也是一种**端到端**吧

